{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6edacef-f40c-4e7f-8174-3d071ede5934",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Summary of Work Done in This Notebook\n",
    "\n",
    "- **Ingested and cleaned raw data tables**: Loaded bronze layer tables (`bronze_students`, `bronze_courses`, `bronze_enrollments`, `bronze_results`) from the database.\n",
    "- **Transformed and standardized data**: Casted columns to appropriate data types, removed rows with missing critical fields, and dropped duplicates for each entity.\n",
    "- **Created Silver Layer Delta Tables**:\n",
    "  - `silver_students`\n",
    "  - `silver_courses`\n",
    "  - `silver_enrollments`\n",
    "  - `silver_results`\n",
    "- **Wrote cleaned DataFrames to Delta tables**: Used overwrite mode and schema evolution to ensure up-to-date, clean data.\n",
    "- **Displayed the contents of each silver table**: Verified data quality and structure after transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01ab38b7-8399-44c1-8044-892430065be0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType, StringType\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Define the schema/database name\n",
    "schema_name = \"kusha_solutions.Jeevan\"\n",
    "\n",
    "# Read the bronze_students table as a DataFrame\n",
    "students_df = spark.table(f\"{schema_name}.bronze_students\")\n",
    "\n",
    "# Clean and transform the students DataFrame\n",
    "students_silver = (\n",
    "    students_df\n",
    "    .withColumn(\"student_id\", F.col(\"student_id\").cast(IntegerType()))      # Cast student_id to Integer\n",
    "    .withColumn(\"name\", F.col(\"name\").cast(StringType()))                   # Cast name to String\n",
    "    .withColumn(\"email\", F.col(\"email\").cast(StringType()))                 # Cast email to String\n",
    "    .withColumn(\"age\", F.col(\"age\").cast(IntegerType()))                    # Cast age to Integer\n",
    "    .withColumn(\"city\", F.col(\"city\").cast(StringType()))                   # Cast city to String\n",
    "    .withColumn(\"department\", F.col(\"department\").cast(StringType()))       # Cast department to String\n",
    "    .dropna(subset=[\"student_id\", \"name\", \"email\",\"age\", \"city\", \"department\"])  # Remove rows with nulls in key fields\n",
    "    .dropDuplicates([\"student_id\", \"email\"])                               # Remove duplicate students based on student_id and email\n",
    ")\n",
    "\n",
    "# Write the cleaned DataFrame to the silver_students Delta table\n",
    "students_silver.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(f\"{schema_name}.silver_students\")\n",
    "\n",
    "print(\"✅ silver_students table created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "839fcefe-a481-450a-9c6c-42d602e9a500",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the schema/database name\n",
    "schema_name = \"kusha_solutions.Jeevan\"\n",
    "\n",
    "# Read the silver_students Delta table into a Spark DataFrame\n",
    "silver_students_df = spark.read.table(f\"{schema_name}.silver_students\")\n",
    "\n",
    "# Display the contents of the silver_students DataFrame\n",
    "display(silver_students_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "822c441a-4e56-4f3f-9b50-b9967e8177a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType, StringType\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Define the target schema/database\n",
    "schema_name = \"kusha_solutions.Jeevan\"\n",
    "\n",
    "# Load the raw bronze_courses table into a DataFrame\n",
    "courses_df = spark.table(f\"{schema_name}.bronze_courses\")\n",
    "\n",
    "# Transform the raw data into a clean silver layer:\n",
    "#   - Cast columns to appropriate data types\n",
    "#   - Remove rows with missing critical fields (course_id, course_name)\n",
    "#   - Drop duplicate records based on course_id\n",
    "courses_silver = (\n",
    "    courses_df\n",
    "    .withColumn(\"course_id\", F.col(\"course_id\").cast(StringType()))   # ensure course_id is string\n",
    "    .withColumn(\"course_name\", F.col(\"course_name\").cast(StringType()))  # ensure course_name is string\n",
    "    .withColumn(\"credits\", F.col(\"credits\").cast(IntegerType()))    # ensure credits is integer\n",
    "    .dropna(subset=[\"course_id\", \"course_name\"])                    # drop rows missing key identifiers\n",
    "    .dropDuplicates([\"course_id\"])                                 # keep only one record per course_id\n",
    ")\n",
    "\n",
    "# Write the cleaned DataFrame to a Delta table in the silver layer,\n",
    "# overwriting any existing data and updating the schema if needed\n",
    "courses_silver.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(f\"{schema_name}.silver_courses\")\n",
    "\n",
    "print(\"✅ silver_courses table created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbd83c5f-2578-4969-89d3-d2e9dd8d1ff8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the schema/database name to use for table references\n",
    "schema_name = \"kusha_solutions.Jeevan\"\n",
    "\n",
    "# Load the cleaned silver_courses Delta table into a Spark DataFrame\n",
    "silver_courses_df = spark.read.table(f\"{schema_name}.silver_courses\")\n",
    "\n",
    "# Display the contents of the silver_courses DataFrame in a tabular format\n",
    "display(silver_courses_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cabe5f0e-e8bd-44eb-9f5c-2456ed494f21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary data types and functions for Spark DataFrame transformations\n",
    "from pyspark.sql.types import IntegerType, StringType, DateType\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Define the schema/database name\n",
    "schema_name = \"kusha_solutions.Jeevan\"\n",
    "\n",
    "# Load the raw bronze_enrollments table into a DataFrame\n",
    "enrollments_df = spark.table(f\"{schema_name}.bronze_enrollments\")\n",
    "\n",
    "# Clean and transform the enrollments DataFrame:\n",
    "#   - Cast columns to appropriate data types\n",
    "#   - Remove rows with missing critical fields (student_id, course_id, status)\n",
    "#   - Drop duplicate records based on student_id and course_id\n",
    "enrollments_silver = (\n",
    "    enrollments_df\n",
    "    .withColumn(\"enrollment_id\", F.col(\"enrollment_id\").cast(IntegerType()))  # Cast enrollment_id to Integer\n",
    "    .withColumn(\"student_id\", F.col(\"student_id\").cast(IntegerType()))        # Cast student_id to Integer\n",
    "    .withColumn(\"course_id\", F.col(\"course_id\").cast(StringType()))           # Cast course_id to String\n",
    "    .withColumn(\"enroll_date\", F.col(\"enroll_date\").cast(DateType()))         # Cast enroll_date to Date\n",
    "    .withColumn(\"status\", F.col(\"status\").cast(StringType()))                 # Cast status to String\n",
    "    .dropna(subset=[\"student_id\", \"course_id\",\"status\"])                     # Remove rows with nulls in key fields\n",
    "    .dropDuplicates([\"student_id\", \"course_id\"])                             # Remove duplicate enrollments per student and course\n",
    ")\n",
    "\n",
    "# Write the cleaned DataFrame to the silver_enrollments Delta table\n",
    "enrollments_silver.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(f\"{schema_name}.silver_enrollments\")\n",
    "\n",
    "# Indicate successful creation of the silver_enrollments table\n",
    "print(\"✅ silver_enrollments table created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c74100c-170f-45c5-8956-88f00958aa4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the schema/database name for table references\n",
    "schema_name = \"kusha_solutions.Jeevan\"\n",
    "\n",
    "# Load the cleaned silver_enrollments Delta table into a Spark DataFrame\n",
    "silver_enrollments_df = spark.read.table(f\"{schema_name}.silver_enrollments\")\n",
    "\n",
    "# Display the contents of the silver_enrollments DataFrame in a tabular format\n",
    "display(silver_enrollments_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f26771e5-69da-4239-8472-cbefa44ad589",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary data types and functions for Spark DataFrame transformations\n",
    "from pyspark.sql.types import IntegerType, StringType, FloatType, DateType\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Define the schema/database name\n",
    "schema_name = \"kusha_solutions.Jeevan\"\n",
    "\n",
    "# Load the raw bronze_results table into a DataFrame\n",
    "results_df = spark.table(f\"{schema_name}.bronze_results\")\n",
    "\n",
    "# Clean and transform the results DataFrame:\n",
    "#   - Cast columns to appropriate data types\n",
    "#   - Remove rows with missing critical fields (enrollment_id, marks, grade)\n",
    "#   - Drop duplicate records based on enrollment_id and result_id\n",
    "results_silver = (\n",
    "    results_df\n",
    "    .withColumn(\"result_id\", F.col(\"result_id\").cast(IntegerType()))         # Cast result_id to Integer\n",
    "    .withColumn(\"enrollment_id\", F.col(\"enrollment_id\").cast(IntegerType())) # Cast enrollment_id to Integer\n",
    "    .withColumn(\"marks\", F.col(\"marks\").cast(FloatType()))                   # Cast marks to Float\n",
    "    .withColumn(\"grade\", F.col(\"grade\").cast(StringType()))                  # Cast grade to String\n",
    "    .dropna(subset=[\"enrollment_id\", \"marks\",\"grade\"])                      # Remove rows with nulls in key fields\n",
    "    .dropDuplicates([\"enrollment_id\",\"result_id\"])                           # Remove duplicate results per enrollment and result\n",
    ")\n",
    "\n",
    "# Write the cleaned DataFrame to the silver_results Delta table\n",
    "results_silver.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(f\"{schema_name}.silver_results\")\n",
    "\n",
    "# Indicate successful creation of the silver_results table\n",
    "print(\"✅ silver_results table created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0e1fb1d-5e18-42ca-b965-e03b334d9bea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the schema/database name for table references\n",
    "schema_name = \"kusha_solutions.Jeevan\"\n",
    "\n",
    "# Read the cleaned silver_results Delta table into a Spark DataFrame\n",
    "silver_results_df = spark.read.table(f\"{schema_name}.silver_results\")\n",
    "\n",
    "# Display the contents of the silver_results DataFrame in a tabular format\n",
    "display(silver_results_df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Silver_Layer",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
