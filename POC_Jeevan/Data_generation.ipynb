{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a015ae2-a129-4e91-a4e5-d1cc83213f2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Schema Creation:**  \n",
    "A new schema was created under the Unity Catalog (`kusha_solutions.Jeevan`) to organize all data tables.\n",
    "\n",
    "**Volume Creation:**  \n",
    "A dedicated volume was established within the schema to store raw CSV files.\n",
    "\n",
    "**Data Generation Using Faker:**  \n",
    "The Faker Python library was utilized to generate synthetic student-related data, with intentional introduction of null values and duplicate records for subsequent data cleaning exercises.\n",
    "\n",
    "**CSV File Creation:**  \n",
    "Four interlinked CSV files were generated to represent a student database system.\n",
    "\n",
    "**Data Storage:**  \n",
    "All CSV files were stored in the Unity Catalog volume at:  \n",
    "`/Volumes/kusha_solutions/jeevan/my_volume/csv_data/raw/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "250f7837-f27f-44ab-9d1c-d9ca475e56e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create the schema 'Jeevan' in the 'kusha_solutions' catalog if it does not already exist\n",
    "spark.sql(\"\"\"\n",
    "    CREATE SCHEMA IF NOT EXISTS kusha_solutions.Jeevan\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6dd701f-1e34-41fa-9846-dff6706b00b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create the volume 'My_volume' in the 'Jeevan' schema under the 'kusha_solutions' catalog if it does not already exist\n",
    "spark.sql(\"\"\"\n",
    "    CREATE VOLUME IF NOT EXISTS kusha_solutions.Jeevan.My_volume\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12b84ade-f9c7-45fa-b1c9-fbc8f575b768",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# STEP 1: Install and import required libraries\n",
    "# ======================================================\n",
    "%pip install faker\n",
    "from faker import Faker  # Library to generate fake data\n",
    "import pandas as pd      # For data manipulation and CSV writing\n",
    "import random            # For random selections\n",
    "import os                # For file path operations\n",
    "\n",
    "# Initialize Faker instance for generating fake data\n",
    "fake = Faker()\n",
    "\n",
    "# Define the path to save the students CSV file in Unity Catalog Volume\n",
    "volume_path = \"/Volumes/kusha_solutions/jeevan/my_volume/csv_data/raw/students/\"\n",
    "\n",
    "# ======================================================\n",
    "# STEP 2: Generate fake students data\n",
    "# ======================================================\n",
    "students = []\n",
    "for i in range(500):\n",
    "        students.append({\n",
    "            \"student_id\": i,  # Unique student ID\n",
    "            \"name\": fake.name(),  # Randomly generated name\n",
    "            \"email\": fake.email(),  # Randomly generated email\n",
    "            \"age\": random.choice([random.randint(18, 25), None]),  # Random age or None\n",
    "            \"city\": random.choice([fake.city(), None]),  # Random city or None\n",
    "            \"department\": random.choice([\"CSE\", \"ECE\", \"MECH\", \"CIVIL\", \"EEE\"])  # Random department\n",
    "        })\n",
    "\n",
    "# Convert the list of students to a DataFrame\n",
    "df = pd.DataFrame(students)\n",
    "\n",
    "# Add 5% duplicate rows to the DataFrame\n",
    "df = pd.concat([df, df.sample(frac=0.05, replace=True)], ignore_index=True)  # Add duplicates\n",
    "\n",
    "# Define the output CSV file name\n",
    "file_name = \"students.csv\"\n",
    "\n",
    "# Construct the full output path for the CSV file\n",
    "output_path = f\"{volume_path}/{file_name}\"\n",
    "\n",
    "# Save the DataFrame as a CSV file at the specified path\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "# Print confirmation messages with file path and row count\n",
    "print(f\"âœ… File '{file_name}' created successfully at: {output_path}\")\n",
    "print(f\"ðŸ§¾ Total Rows: {len(df)} (includes nulls + duplicates)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c3a2554-d8ef-4d26-b9ed-debd93727e43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read the students CSV files from the specified Unity Catalog volume path into a Spark DataFrame\n",
    "students_df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"inferSchema\", True) \\\n",
    "    .load(\"/Volumes/kusha_solutions/jeevan/my_volume/csv_data/raw/students/students.csv\")\n",
    "\n",
    "# Display the DataFrame in a tabular format\n",
    "display(students_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea8a163b-5cf3-4f19-8c64-1f8fe7f97765",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# STEP 1: Install and import required libraries\n",
    "# ======================================================\n",
    "%pip install faker\n",
    "from faker import Faker  # Library to generate fake data (not used here, but for consistency)\n",
    "import pandas as pd      # For data manipulation and CSV writing\n",
    "import random            # For random selections\n",
    "import os                # For file path operations\n",
    "\n",
    "# Initialize Faker instance (not used in this cell, but for consistency)\n",
    "fake = Faker()\n",
    "\n",
    "# ======================================================\n",
    "# STEP 2: Define the path to save the courses CSV file in Unity Catalog Volume\n",
    "# ======================================================\n",
    "volume_path = \"/Volumes/kusha_solutions/jeevan/my_volume/csv_data/raw/courses/\"\n",
    "\n",
    "# ======================================================\n",
    "# STEP 3: Create a list of course dictionaries\n",
    "# ======================================================\n",
    "courses = [\n",
    "        {\"course_id\": \"C101\", \"course_name\": \"Python\", \"credits\": 3},\n",
    "        {\"course_id\": \"C102\", \"course_name\": \"Data Science\", \"credits\": 4},\n",
    "        {\"course_id\": \"C103\", \"course_name\": \"AI & ML\", \"credits\": 4},\n",
    "        {\"course_id\": \"C104\", \"course_name\": \"Database Systems\", \"credits\": 3},\n",
    "        {\"course_id\": \"C105\", \"course_name\": \"Computer Networks\", \"credits\": 3},\n",
    "    ]\n",
    "\n",
    "# Convert the list of courses to a DataFrame\n",
    "df = pd.DataFrame(courses)\n",
    "\n",
    "# Add 10% duplicate rows to the DataFrame\n",
    "df = pd.concat([df, df.sample(frac=0.1, replace=True)], ignore_index=True)\n",
    "\n",
    "# Define the output CSV file name\n",
    "file_name = \"courses.csv\"\n",
    "\n",
    "# Construct the full output path for the CSV file\n",
    "output_path = f\"{volume_path}/{file_name}\"\n",
    "\n",
    "# Save the DataFrame as a CSV file at the specified path\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "# Print confirmation messages with file path and row count\n",
    "print(f\"âœ… File '{file_name}' created successfully at: {output_path}\")\n",
    "print(f\"ðŸ§¾ Total Rows: {len(df)} (includes nulls + duplicates)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7b4276d-50fe-47f3-82fe-fd6e8d7df508",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# STEP 1: Import required libraries for data generation and file operations\n",
    "# ======================================================\n",
    "from faker import Faker\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "\n",
    "fake = Faker()\n",
    "\n",
    "# ======================================================\n",
    "# STEP 2: Set the path where the enrollments CSV will be saved\n",
    "# ======================================================\n",
    "volume_path = \"/Volumes/kusha_solutions/jeevan/my_volume/csv_data/raw/enrollments\"\n",
    "os.makedirs(volume_path, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "\n",
    "# ======================================================\n",
    "# STEP 3: Load existing students and courses data from CSV files\n",
    "# ======================================================\n",
    "students_df = pd.read_csv(\"/Volumes/kusha_solutions/jeevan/my_volume/csv_data/raw/students/students.csv\")\n",
    "courses_df = pd.read_csv(\"/Volumes/kusha_solutions/jeevan/my_volume/csv_data/raw/courses/courses.csv\")\n",
    "\n",
    "# ======================================================\n",
    "# STEP 4: Generate fake enrollments data with unique enrollment IDs\n",
    "# ======================================================\n",
    "\n",
    "enrollments = []\n",
    "\n",
    "for i in range(1, 550):\n",
    "    student = students_df.sample(1).iloc[0]  # Randomly select a student\n",
    "    course = courses_df.sample(1).iloc[0]    # Randomly select a course\n",
    "    enrollments.append({\n",
    "        \"enrollment_id\": i,  # Unique sequential enrollment ID\n",
    "        \"student_id\": student[\"student_id\"],  # Reference to student\n",
    "        \"course_id\": course[\"course_id\"],     # Reference to course\n",
    "        \"enroll_date\": fake.date_this_year().strftime(\"%Y-%m-%d\"),  # Random enrollment date in current year\n",
    "        \"status\": random.choice([\"Active\", \"Completed\", \"Dropped\", None])  # Random status, some nulls\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(enrollments)\n",
    "\n",
    "# Add 5% duplicate rows to simulate real-world data issues\n",
    "df = pd.concat([df, df.sample(frac=0.05, replace=True)], ignore_index=True)\n",
    "\n",
    "# ======================================================\n",
    "# STEP 5: Save the generated enrollments data as a CSV file\n",
    "# ======================================================\n",
    "file_name = \"enrollments.csv\"\n",
    "output_path = f\"{volume_path}/{file_name}\"\n",
    "\n",
    "df.to_csv(output_path, index=False)  # Write DataFrame to CSV\n",
    "print(f\"âœ… CSV '{file_name}' created successfully at: {output_path}\")\n",
    "print(f\"ðŸ§¾ Total Rows: {len(df)} (includes nulls + duplicates)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5a2c18b-4fc2-4ce9-b0fb-f15b5ca68971",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read the enrollments CSV file from the specified Unity Catalog volume path into a Spark DataFrame\n",
    "df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"inferSchema\", True) \\\n",
    "    .load(\"/Volumes/kusha_solutions/jeevan/my_volume/csv_data/raw/enrollments/enrollments.csv\")\n",
    "\n",
    "# Display the DataFrame in a tabular format\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbd14fb4-160c-41d1-b4e9-96029d1d9ff1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read the courses CSV file from the specified Unity Catalog volume path into a Spark DataFrame\n",
    "df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"inferSchema\", True) \\\n",
    "    .load(\"/Volumes/kusha_solutions/jeevan/my_volume/csv_data/raw/courses/courses.csv\")\n",
    "\n",
    "# Display the DataFrame in a tabular format\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb3f22ac-98fb-468a-9356-2316392c1329",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# STEP 1: Import libraries needed for data generation and file operations\n",
    "# ======================================================\n",
    "%pip install faker\n",
    "from faker import Faker\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "\n",
    "fake = Faker()\n",
    "\n",
    "# ======================================================\n",
    "# STEP 2: Define the path where the results CSV will be saved in the Unity Catalog volume\n",
    "# ======================================================\n",
    "volume_path = \"/Volumes/kusha_solutions/jeevan/my_volume/csv_data/raw/results\"\n",
    "\n",
    "# ======================================================\n",
    "# STEP 3: Read the enrollments CSV file to get existing enrollment records\n",
    "# ======================================================\n",
    "enrollments_df = pd.read_csv(\"/Volumes/kusha_solutions/jeevan/my_volume/csv_data/raw/enrollments/enrollments.csv\")\n",
    "\n",
    "# ======================================================\n",
    "# STEP 4: Generate a fake results dataset linked to enrollments\n",
    "# ======================================================\n",
    "num_results = 400  # Number of result records to generate\n",
    "results = []\n",
    "\n",
    "# Randomly sample enrollments to assign results to\n",
    "sampled_enrollments = enrollments_df.sample(num_results, replace=False)  # Pick random enrollments\n",
    "\n",
    "# Create result records with random marks and grades, some with nulls\n",
    "for i, row in enumerate(sampled_enrollments.itertuples(), start=1):\n",
    "    results.append({\n",
    "        \"result_id\": i,  # Sequential unique result ID\n",
    "        \"enrollment_id\": row.enrollment_id,  # Reference to enrollment\n",
    "        \"marks\": random.choice([round(random.uniform(30, 100), 2), None]),  # Random marks or None\n",
    "        \"grade\": random.choice([\"A\", \"B\", \"C\", \"D\", \"F\", None])  # Random grade or None\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Add 10% duplicate rows to simulate real-world data issues\n",
    "df = pd.concat([df, df.sample(frac=0.1, replace=True)], ignore_index=True)\n",
    "\n",
    "# ======================================================\n",
    "# STEP 5: Save the generated results data as a CSV file in the specified volume path\n",
    "# ======================================================\n",
    "file_name = \"results.csv\"\n",
    "output_path = f\"{volume_path}/{file_name}\"\n",
    "\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "# Print confirmation messages with file path and row count\n",
    "print(f\"âœ… CSV '{file_name}' created successfully at: {output_path}\")\n",
    "print(f\"ðŸ§¾ Total Rows: {len(df)} (includes nulls + duplicates)\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Data_generation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
